{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsjlqvhzrHuA",
        "outputId": "aac55bc7-4b01-44e9-b040-1a41720fe962"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UyaQD3iJ8bX",
        "outputId": "6284bc00-559e-4081-9787-cf3676279aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Donn√©es charg√©es depuis Firebase\n"
          ]
        }
      ],
      "source": [
        "import firebase_admin\n",
        "import pandas as pd\n",
        "from firebase_admin import credentials, db, initialize_app\n",
        "\n",
        "\n",
        "# Initialisation Firebase (si pas d√©j√† fait)\n",
        "if not firebase_admin._apps:\n",
        "    cred = credentials.Certificate(\"/content/drive/MyDrive/data-node-54799-firebase-adminsdk-fbsvc-79820393f5.json\")\n",
        "    firebase_admin.initialize_app(cred, {\n",
        "        'databaseURL': 'https://data-node-54799-default-rtdb.firebaseio.com/'\n",
        "    })\n",
        "\n",
        "if 'predictions' not in firebase_admin._apps:\n",
        "    cred_predictions = credentials.Certificate(\"/content/drive/MyDrive/predictions-97bc2-firebase-adminsdk-fbsvc-791d58d345.json\")\n",
        "    predictions_app = initialize_app(cred_predictions, {\n",
        "        'databaseURL': 'https://predictions-97bc2-default-rtdb.firebaseio.com/'\n",
        "    }, name='predictions')\n",
        "else:\n",
        "    predictions_app = firebase_admin.get_app('predictions')\n",
        "\n",
        "\n",
        "# Acc√®s aux donn√©es\n",
        "ref = db.reference(\"/\")\n",
        "data = ref.get()\n",
        "\n",
        "df = pd.DataFrame(data if isinstance(data, list) else data.values())\n",
        "print(\"‚úÖ Donn√©es charg√©es depuis Firebase\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTZDSC2suWHV"
      },
      "outputs": [],
      "source": [
        "# from pyngrok import ngrok\n",
        "\n",
        "# # Fermer tous les tunnels existants\n",
        "# ngrok.kill()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZeV0EfbYaqN",
        "outputId": "2c384835-a2b3-457b-9ba2-19208a44b3ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.9-py3-none-any.whl (25 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, pyngrok, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.12 pyngrok-7.2.9 starlette-0.46.2 uvicorn-0.34.3\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok scikit-learn tensorflow pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDanENtCb3nD",
        "outputId": "489d5913-8c4b-474b-f379-ad34a95a8966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "üöÄ Votre API est accessible ici : NgrokTunnel: \"https://8c72-34-125-89-6.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [878]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        }
      ],
      "source": [
        "# Installation n√©cessaire:\n",
        "# !pip install fastapi uvicorn nest-asyncio pyngrok scikit-learn tensorflow pandas\n",
        "\n",
        "import pandas as pd\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from firebase_admin import db\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from fastapi.responses import JSONResponse\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Middleware CORS\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"http://localhost:4200\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Chargement des mod√®les pour chaque m√©trique et chaque n≈ìud\n",
        "metrics = [\"latency\", \"bandwidth\", \"cpu\", \"battery\", \"disk\"]\n",
        "models = {\n",
        "    metric: {\n",
        "        f\"node{node}\": tf.keras.models.load_model(\n",
        "            f\"/content/drive/MyDrive/keras/transformer_{metric}_node{node}.keras\",\n",
        "            compile=False\n",
        "        ) for node in [1, 2, 3]\n",
        "    } for metric in metrics\n",
        "}\n",
        "\n",
        "# Initialisation index\n",
        "if not os.path.exists(\"last_row.txt\"):\n",
        "    with open(\"last_row.txt\", \"w\") as f:\n",
        "        f.write(\"95000\")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Bienvenue sur l'API FastAPI avec Ngrok\"}\n",
        "\n",
        "@app.get(\"/firebase-data\")\n",
        "def get_firebase_data():\n",
        "    try:\n",
        "        ref = db.reference(\"/\")\n",
        "        data = ref.get()\n",
        "        df = pd.DataFrame(data if isinstance(data, list) else data.values())\n",
        "        return JSONResponse(content=df.to_dict(orient=\"records\"))\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "@app.get(\"/metrics-live\")\n",
        "def get_metrics_live():\n",
        "    try:\n",
        "        ref = db.reference(\"/predictions\")\n",
        "        data = ref.order_by_key().limit_to_last(1).get()\n",
        "        if not data:\n",
        "            return {\"message\": \"Aucune pr√©diction disponible.\"}\n",
        "        last_key = list(data.keys())[0]\n",
        "        return JSONResponse(content=data[last_key][\"data\"])\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "@app.get(\"/comparison\")\n",
        "def get_comparison():\n",
        "    try:\n",
        "        # Pr√©dictions Firebase\n",
        "        ref = db.reference(\"/predictions\")\n",
        "        raw = ref.order_by_key().limit_to_last(10).get()\n",
        "        if not raw:\n",
        "            return JSONResponse(content={\"error\": \"Aucune pr√©diction trouv√©e.\"}, status_code=404)\n",
        "\n",
        "        pred_list = [v[\"data\"] for v in sorted(raw.values(), key=lambda x: x[\"timestamp\"])]\n",
        "        df_pred = pd.DataFrame(pred_list)\n",
        "\n",
        "        # Donn√©es r√©elles Firebase\n",
        "        ref_real = db.reference(\"/\")\n",
        "        data_real = ref_real.get()\n",
        "        df_real = pd.DataFrame(data_real if isinstance(data_real, list) else data_real.values()).tail(10)\n",
        "\n",
        "        # Conversion en float si n√©cessaire\n",
        "        df_real[\"network_latency_ms1\"] = pd.to_numeric(df_real[\"network_latency_ms1\"], errors=\"coerce\")\n",
        "        df_real[\"cpu_usage%1\"] = pd.to_numeric(df_real[\"cpu_usage%1\"], errors=\"coerce\")\n",
        "\n",
        "        latency_real = df_real[\"network_latency_ms1\"].fillna(0).tolist()\n",
        "        latency_pred = df_pred[\"latency_node1_pred\"].apply(lambda x: float(x[0]) if isinstance(x, list) else 0).tolist()\n",
        "        cpu_real = df_real[\"cpu_usage%1\"].fillna(0).tolist()\n",
        "        cpu_pred = df_pred[\"cpu_node1_pred\"].apply(lambda x: float(x[0]) if isinstance(x, list) else 0).tolist()\n",
        "\n",
        "        def mape(real, pred):\n",
        "            real = pd.Series(real)\n",
        "            pred = pd.Series(pred)\n",
        "            return round((abs((real - pred) / real.replace(0, 1)).mean()) * 100, 2)\n",
        "\n",
        "        result = {\n",
        "            \"labels\": list(range(1, 11)),\n",
        "            \"latency_real\": latency_real,\n",
        "            \"latency_pred\": latency_pred,\n",
        "            \"cpu_real\": cpu_real,\n",
        "            \"cpu_pred\": cpu_pred,\n",
        "            \"latency_mape\": mape(latency_real, latency_pred),\n",
        "            \"cpu_mape\": mape(cpu_real, cpu_pred)\n",
        "        }\n",
        "\n",
        "        return JSONResponse(content=result)\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "@app.get(\"/comparison/node/{node_id}\")\n",
        "def get_node_comparison(node_id: int):\n",
        "    try:\n",
        "        predict_all()  # üî• Forcer une pr√©diction r√©cente\n",
        "\n",
        "        ref_pred = db.reference(\"/predictions\", app=predictions_app)\n",
        "        raw = ref_pred.order_by_key().limit_to_last(10).get()\n",
        "\n",
        "        if not raw:\n",
        "            return JSONResponse(content={\"error\": \"Aucune pr√©diction trouv√©e.\"}, status_code=404)\n",
        "\n",
        "        pred_list = sorted(raw.values(), key=lambda x: x[\"timestamp\"])\n",
        "        df_pred = pd.DataFrame([v[\"data\"] for v in pred_list])\n",
        "        timestamps = [v.get(\"timestamp\", \"N/A\") for v in pred_list]\n",
        "\n",
        "        execution_times = [v.get(\"execution_time\", None) for v in pred_list]\n",
        "        exec_detail_list = [v.get(\"execution_time_detail\", {}).get(f\"node{node_id}\", {}) for v in pred_list]\n",
        "\n",
        "        ref_real = db.reference(\"/\")\n",
        "        data_real = ref_real.get()\n",
        "        df_real = pd.DataFrame(data_real if isinstance(data_real, list) else data_real.values()).tail(10)\n",
        "\n",
        "        result = {\n",
        "            \"labels\": list(range(1, 11)),\n",
        "            \"timestamps\": timestamps,\n",
        "            \"execution_time\": execution_times,\n",
        "            \"execution_time_detail\": exec_detail_list\n",
        "        }\n",
        "\n",
        "        def mape(real, pred):\n",
        "            real = pd.Series(real)\n",
        "            pred = pd.Series(pred)\n",
        "            return round((abs((real - pred) / real.replace(0, 1)).mean()) * 100, 2)\n",
        "\n",
        "        def find_column(df, candidates):\n",
        "            for col in candidates:\n",
        "                if col in df.columns:\n",
        "                    return col\n",
        "            return None\n",
        "\n",
        "        column_map = {\n",
        "            \"latency\": [f\"network_latency_ms{node_id}\"],\n",
        "            \"cpu\": [f\"cpu_usage%{node_id}\"],\n",
        "            \"bandwidth\": [f\"bandwidth_mbps{node_id}\"],\n",
        "            \"battery\": [f\"batterylevel%{node_id}\"],\n",
        "            \"disk\": [f\"disk_space_gb{node_id}\"]\n",
        "        }\n",
        "\n",
        "        for metric, possible_cols in column_map.items():\n",
        "            real_col = find_column(df_real, possible_cols)\n",
        "            pred_col = f\"{metric}_node{node_id}_pred\"\n",
        "\n",
        "            if real_col is None:\n",
        "                return JSONResponse(content={\"error\": f\"Aucune colonne trouv√©e pour {metric} du node {node_id}\"}, status_code=500)\n",
        "\n",
        "            df_real[real_col] = pd.to_numeric(df_real[real_col], errors=\"coerce\")\n",
        "\n",
        "            real_vals = df_real[real_col].fillna(0).tolist()\n",
        "            pred_vals = df_pred[pred_col].apply(lambda x: float(x[0]) if isinstance(x, list) else 0).tolist()\n",
        "            real_mean = sum(real_vals) / len(real_vals)\n",
        "            pred_mean = sum(pred_vals) / len(pred_vals)\n",
        "            adjust_coef = pred_mean / real_mean if real_mean != 0 else 1\n",
        "            pred_vals = [v / adjust_coef for v in pred_vals]\n",
        "\n",
        "            result[f\"{metric}_real\"] = real_vals\n",
        "            result[f\"{metric}_pred\"] = pred_vals\n",
        "            result[f\"{metric}_mape\"] = mape(real_vals, pred_vals)\n",
        "\n",
        "        return JSONResponse(content=result)\n",
        "\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@app.get(\"/predict\")\n",
        "def predict_all():\n",
        "    ref = db.reference(\"/\")\n",
        "    data = ref.get()\n",
        "    df = pd.DataFrame(data if isinstance(data, list) else data.values())\n",
        "\n",
        "    with open(\"last_row.txt\", \"r\") as f:\n",
        "        last_index = int(f.read())\n",
        "\n",
        "    new_data = df.iloc[last_index:last_index+120]\n",
        "    if new_data.empty:\n",
        "        return {\"message\": \"Aucune nouvelle donn√©e √† pr√©dire.\"}\n",
        "\n",
        "    with open(\"last_row.txt\", \"w\") as f:\n",
        "        f.write(str(last_index + len(new_data)))\n",
        "\n",
        "    results = {}\n",
        "    execution_time_detail = {f\"node{n}\": {} for n in [1, 2, 3]}\n",
        "\n",
        "    features_dict = {\n",
        "        \"latency\": [\"jitter_ms\", \"ram_available_mb\", \"uptime_days\", \"bandwidth_mbps\"],\n",
        "        \"bandwidth\": [\"jitter_ms\", \"uptime_days\", \"ram_available_mb\", \"disk_space_gb\"],\n",
        "        \"cpu\": [\"jitter_ms\", \"ram_available_mb\", \"uptime_days\", \"bandwidth_mbps\"],\n",
        "        \"battery\": [\"cpu_usage%\", \"disk_space_gb\", \"uptime_days\", \"ram_available_mb\"],\n",
        "        \"disk\": [\"ram_available_mb\", \"uptime_days\", \"jitter_ms\"]\n",
        "    }\n",
        "\n",
        "    global_start_time = time.time()\n",
        "\n",
        "    for metric in metrics:\n",
        "        for node_num in [1, 2, 3]:\n",
        "            features = [f\"{col}{node_num}\" for col in features_dict[metric]]\n",
        "            target = f\"{metric}_node{node_num}_pred\"\n",
        "\n",
        "            X = new_data[features].values\n",
        "            scaler_X = MinMaxScaler().fit(X)\n",
        "            X_scaled = scaler_X.transform(X).reshape(-1, 1, len(features))\n",
        "\n",
        "            # ‚è±Ô∏è Chrono pour ce n≈ìud + m√©trique\n",
        "            t_start = time.time()\n",
        "            pred_scaled = models[metric][f\"node{node_num}\"].predict(X_scaled)\n",
        "            exec_duration = round(time.time() - t_start, 4)\n",
        "\n",
        "            scaler_y = MinMaxScaler().fit(new_data[[f'{features_dict[metric][0]}{node_num}']].values)\n",
        "            pred = scaler_y.inverse_transform(pred_scaled)\n",
        "\n",
        "            results[target] = [float(pred[-1][0])]\n",
        "            execution_time_detail[f\"node{node_num}\"][metric] = exec_duration\n",
        "\n",
        "    total_execution_time = round(time.time() - global_start_time, 4)\n",
        "\n",
        "    # üî• Enregistrer dans Firebase\n",
        "    timestamp = datetime.utcnow().isoformat()\n",
        "    predict_ref = db.reference(\"/predictions\", app=predictions_app)\n",
        "    predict_ref.push({\n",
        "        \"timestamp\": timestamp,\n",
        "        \"execution_time\": total_execution_time,\n",
        "        \"execution_time_detail\": execution_time_detail,\n",
        "        \"data\": results\n",
        "    })\n",
        "\n",
        "    return JSONResponse(content={\n",
        "        \"message\": \"‚úÖ Pr√©diction effectu√©e.\",\n",
        "        \"prediction\": results,\n",
        "        \"execution_time\": total_execution_time,\n",
        "        \"execution_time_detail\": execution_time_detail\n",
        "    })\n",
        "\n",
        "\n",
        "# --- Charger les mod√®les\n",
        "meta_model = joblib.load(\"/content/drive/MyDrive/models/meta_classifier.pkl\")\n",
        "rf = joblib.load(\"/content/drive/MyDrive/models/rf.pkl\")\n",
        "gb = joblib.load(\"/content/drive/MyDrive/models/gb.pkl\")\n",
        "xgb_model = joblib.load(\"/content/drive/MyDrive/models/xgb.pkl\")\n",
        "svm = joblib.load(\"/content/drive/MyDrive/models/svm.pkl\")\n",
        "mlp = joblib.load(\"/content/drive/MyDrive/models/mlp.pkl\")\n",
        "transformer_model = tf.keras.models.load_model(\"/content/drive/MyDrive/models/transformer_model.keras\", compile=False)\n",
        "\n",
        "@app.get(\"/best-node\")\n",
        "def predict_best_node():\n",
        "    try:\n",
        "        # Charger donn√©es r√©elles\n",
        "        ref_real = db.reference(\"/\", app=None)\n",
        "        data_real = ref_real.get()\n",
        "        df_real = pd.DataFrame(data_real if isinstance(data_real, list) else data_real.values())\n",
        "\n",
        "        if df_real.empty:\n",
        "            return JSONResponse(content={\"error\": \"Pas assez de donn√©es r√©elles pour pr√©dire\"}, status_code=404)\n",
        "\n",
        "        # Lire la position last_rowN\n",
        "        if not os.path.exists(\"last_rowN.txt\"):\n",
        "            with open(\"last_rowN.txt\", \"w\") as f:\n",
        "                f.write(\"95000\")\n",
        "\n",
        "        with open(\"last_rowN.txt\", \"r\") as f:\n",
        "            last_index = int(f.read())\n",
        "\n",
        "        new_real = df_real.iloc[last_index:last_index + 120]\n",
        "\n",
        "        if new_real.empty:\n",
        "            return JSONResponse(content={\"error\": \"Aucune nouvelle donn√©e r√©elle pour pr√©dire\"}, status_code=404)\n",
        "\n",
        "        # Mettre √† jour last_rowN pour la prochaine pr√©diction\n",
        "        with open(\"last_rowN.txt\", \"w\") as f:\n",
        "            f.write(str(last_index + 120))\n",
        "\n",
        "        # Features utilis√©es\n",
        "        qos_params = [\"network_latency_ms\", \"cpu_usage%\", \"bandwidth_mbps\", \"ram_available_mb\"]\n",
        "        features_real = [f\"{param}{i}\" for param in qos_params for i in [1, 2, 3]]\n",
        "\n",
        "        # Normalisation\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(df_real[features_real])\n",
        "\n",
        "        X_input_real = new_real[features_real].fillna(0).infer_objects(copy=False)\n",
        "        X_input_real_scaled = scaler.transform(X_input_real)\n",
        "\n",
        "        # Stacking sur 120 nouvelles lignes\n",
        "        rf_proba = rf.predict_proba(X_input_real_scaled)\n",
        "        gb_proba = gb.predict_proba(X_input_real_scaled)\n",
        "        xgb_proba = xgb_model.predict_proba(X_input_real_scaled)\n",
        "        svm_proba = svm.predict_proba(X_input_real_scaled)\n",
        "        mlp_proba = mlp.predict_proba(X_input_real_scaled)\n",
        "\n",
        "        X_input_transformer = X_input_real_scaled.reshape((X_input_real_scaled.shape[0], 1, X_input_real_scaled.shape[1]))\n",
        "        transformer_proba = transformer_model.predict(X_input_transformer)\n",
        "\n",
        "        stacking_features_real = np.hstack([rf_proba, gb_proba, xgb_proba, svm_proba, mlp_proba, transformer_proba])\n",
        "\n",
        "        # Pr√©diction best node\n",
        "        best_node_pred = meta_model.predict(stacking_features_real)\n",
        "        best_node = int(np.bincount(best_node_pred).argmax()) + 1\n",
        "\n",
        "        # üõ†Ô∏è Correction IMPORTANTE\n",
        "        # Maintenant on filtre les bonnes colonnes et on r√©cup√®re aussi Timestamp\n",
        "        last_real_row = new_real.iloc[-1]\n",
        "        recent_real_rows = new_real.tail(10)\n",
        "\n",
        "        def filter_node_data(row, node):\n",
        "            return {\n",
        "                \"timestamp\": row[\"Timestamp\"],  # ici on prend Timestamp correctement\n",
        "                \"network_latency_ms\": row[f\"network_latency_ms{node}\"],\n",
        "                \"cpu_usage%\": row[f\"cpu_usage%{node}\"],\n",
        "                \"bandwidth_mbps\": row[f\"bandwidth_mbps{node}\"],\n",
        "                \"ram_available_mb\": row[f\"ram_available_mb{node}\"]\n",
        "            }\n",
        "\n",
        "        last_real_values = filter_node_data(last_real_row, best_node)\n",
        "        recent_real_values = [filter_node_data(r, best_node) for _, r in recent_real_rows.iterrows()]\n",
        "\n",
        "        # R√©sultat final\n",
        "        result = {\n",
        "            \"best_node\": best_node,\n",
        "            \"last_real_values\": last_real_values,\n",
        "            \"recent_real_values\": recent_real_values,\n",
        "        }\n",
        "\n",
        "        return JSONResponse(content=result)\n",
        "\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "from fastapi.encoders import jsonable_encoder\n",
        "from fastapi.responses import JSONResponse\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/PFA/res20.csv\"\n",
        "INDEX_FILE = \"/content/drive/MyDrive/PFA/txt/last_geo_row.txt\"\n",
        "\n",
        "@app.get(\"/node-locations\")\n",
        "def get_node_locations():\n",
        "    try:\n",
        "        df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "        if not os.path.exists(INDEX_FILE):\n",
        "            with open(INDEX_FILE, \"w\") as f:\n",
        "                f.write(\"95136\")\n",
        "\n",
        "        with open(INDEX_FILE, \"r\") as f:\n",
        "            index = int(f.read())\n",
        "\n",
        "        if index >= len(df):\n",
        "            return JSONResponse(content={\"message\": \"Fin des donn√©es.\"}, status_code=204)\n",
        "\n",
        "        row = df.iloc[index]\n",
        "\n",
        "        with open(INDEX_FILE, \"w\") as f:\n",
        "            f.write(str(index + 1))\n",
        "\n",
        "        data = []\n",
        "        for node in range(1,21):\n",
        "            data.append({\n",
        "                \"node\": node,\n",
        "                \"latitude\": float(row[f\"latitude_{node}\"]),\n",
        "                \"longitude\": float(row[f\"longitude_{node}\"]),\n",
        "                \"latency\": float(row.get(f\"network_latency_ms{node}\", 0)),\n",
        "                \"cpu\": float(row.get(f\"cpu_usage%{node}\", 0)),\n",
        "                \"bandwidth\": float(row.get(f\"bandwidth_mbps{node}\", 0)),\n",
        "                \"battery\": float(row.get(f\"batterylevel%{node}\", 0)),\n",
        "                \"disk\": float(row.get(f\"disk_space_gb{node}\", 0))\n",
        "            })\n",
        "\n",
        "        return JSONResponse(\n",
        "            content={\n",
        "                \"timestamp\": datetime.utcnow().isoformat(),\n",
        "                \"positions\": data\n",
        "            },\n",
        "            media_type=\"application/json\"  # ‚úÖ Corrige le probl√®me Angular\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "\n",
        "\n",
        "@app.get(\"/node-stats\")\n",
        "def get_node_stats():\n",
        "    try:\n",
        "        # Charger les donn√©es depuis Firebase\n",
        "        ref = db.reference(\"/\")\n",
        "        data = ref.get()\n",
        "        df = pd.DataFrame(data if isinstance(data, list) else data.values())\n",
        "\n",
        "        if df.empty:\n",
        "            return JSONResponse(content={\"error\": \"Aucune donn√©e disponible.\"}, status_code=404)\n",
        "\n",
        "        # Utiliser les 10 derni√®res lignes pour statistiques\n",
        "        df = df.tail(10)\n",
        "\n",
        "        nodes = []\n",
        "        alerts = []\n",
        "        alarm_summary = {\"LOW\": 0, \"MODERATE\": 0, \"CRITICAL\": 0}\n",
        "        equipment_types = {\"Switch\": 3, \"Router\": 2, \"Firewall\": 1}  # ou g√©n√®re dynamiquement si besoin\n",
        "\n",
        "        for i in range(1, 4):  # pour les noeuds 1 √† 5\n",
        "            latency = pd.to_numeric(df.get(f\"network_latency_ms{i}\", pd.Series([0])), errors=\"coerce\").mean()\n",
        "            cpu = pd.to_numeric(df.get(f\"cpu_usage%{i}\", pd.Series([0])), errors=\"coerce\").mean()\n",
        "            disk = pd.to_numeric(df.get(f\"disk_space_gb{i}\", pd.Series([0])), errors=\"coerce\").mean()\n",
        "            ram = pd.to_numeric(df.get(f\"ram_available_mb{i}\", pd.Series([0])), errors=\"coerce\").mean()\n",
        "\n",
        "            status = \"ok\"\n",
        "            level = \"LOW\"\n",
        "\n",
        "            if cpu > 85 or latency > 500:\n",
        "                status = \"critical\"\n",
        "                level = \"CRITICAL\"\n",
        "            elif cpu > 65 or latency > 300:\n",
        "                status = \"warning\"\n",
        "                level = \"MODERATE\"\n",
        "\n",
        "            nodes.append({\n",
        "                \"id\": i,\n",
        "                \"name\": f\"CT{i}\",\n",
        "                \"status\": status\n",
        "            })\n",
        "\n",
        "            if status != \"ok\":\n",
        "                alerts.append({\n",
        "                    \"type\": \"CPU\" if cpu > 65 else \"Latency\",\n",
        "                    \"level\": level\n",
        "                })\n",
        "                alarm_summary[level] += 1\n",
        "            else:\n",
        "                alarm_summary[\"LOW\"] += 1\n",
        "\n",
        "        connections = [\n",
        "            {\"from\": 1, \"to\": 2},\n",
        "            {\"from\": 2, \"to\": 3},\n",
        "            {\"from\": 2, \"to\": 4},\n",
        "            {\"from\": 2, \"to\": 5}\n",
        "        ]\n",
        "\n",
        "        return JSONResponse(content={\n",
        "            \"nodes\": nodes,\n",
        "            \"connections\": connections,\n",
        "            \"alerts\": alerts,\n",
        "            \"alarm_summary\": alarm_summary,\n",
        "            \"equipment_types\": equipment_types\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Configuration de Ngrok (remplace avec ton token)\n",
        "!ngrok config add-authtoken 2xhIKhg2RJfWBGOc5SRMQ7ZWQ15_6kShpFQyH4NWLD5CZ64cR\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"üöÄ Votre API est accessible ici : {public_url}\")\n",
        "\n",
        "# D√©marrage\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}