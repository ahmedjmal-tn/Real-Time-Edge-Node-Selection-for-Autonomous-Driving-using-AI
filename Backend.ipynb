{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsjlqvhzrHuA",
        "outputId": "aac55bc7-4b01-44e9-b040-1a41720fe962"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UyaQD3iJ8bX",
        "outputId": "6284bc00-559e-4081-9787-cf3676279aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Données chargées depuis Firebase\n"
          ]
        }
      ],
      "source": [
        "import firebase_admin\n",
        "import pandas as pd\n",
        "from firebase_admin import credentials, db, initialize_app\n",
        "\n",
        "\n",
        "# Initialisation Firebase (si pas déjà fait)\n",
        "if not firebase_admin._apps:\n",
        "    cred = credentials.Certificate(\"/content/drive/MyDrive/data-node-54799-firebase-adminsdk-fbsvc-79820393f5.json\")\n",
        "    firebase_admin.initialize_app(cred, {\n",
        "        'databaseURL': 'https://data-node-54799-default-rtdb.firebaseio.com/'\n",
        "    })\n",
        "\n",
        "if 'predictions' not in firebase_admin._apps:\n",
        "    cred_predictions = credentials.Certificate(\"/content/drive/MyDrive/predictions-97bc2-firebase-adminsdk-fbsvc-791d58d345.json\")\n",
        "    predictions_app = initialize_app(cred_predictions, {\n",
        "        'databaseURL': 'https://predictions-97bc2-default-rtdb.firebaseio.com/'\n",
        "    }, name='predictions')\n",
        "else:\n",
        "    predictions_app = firebase_admin.get_app('predictions')\n",
        "\n",
        "\n",
        "# Accès aux données\n",
        "ref = db.reference(\"/\")\n",
        "data = ref.get()\n",
        "\n",
        "df = pd.DataFrame(data if isinstance(data, list) else data.values())\n",
        "print(\"✅ Données chargées depuis Firebase\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTZDSC2suWHV"
      },
      "outputs": [],
      "source": [
        "# from pyngrok import ngrok\n",
        "\n",
        "# # Fermer tous les tunnels existants\n",
        "# ngrok.kill()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZeV0EfbYaqN",
        "outputId": "2c384835-a2b3-457b-9ba2-19208a44b3ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.9-py3-none-any.whl (25 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, pyngrok, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.12 pyngrok-7.2.9 starlette-0.46.2 uvicorn-0.34.3\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok scikit-learn tensorflow pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDanENtCb3nD",
        "outputId": "489d5913-8c4b-474b-f379-ad34a95a8966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "🚀 Votre API est accessible ici : NgrokTunnel: \"https://8c72-34-125-89-6.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [878]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        }
      ],
      "source": [
        "# Installation nécessaire:\n",
        "# !pip install fastapi uvicorn nest-asyncio pyngrok scikit-learn tensorflow pandas\n",
        "\n",
        "import pandas as pd\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from firebase_admin import db\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from fastapi.responses import JSONResponse\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Middleware CORS\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"http://localhost:4200\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Chargement des modèles pour chaque métrique et chaque nœud\n",
        "metrics = [\"latency\", \"bandwidth\", \"cpu\", \"battery\", \"disk\"]\n",
        "models = {\n",
        "    metric: {\n",
        "        f\"node{node}\": tf.keras.models.load_model(\n",
        "            f\"/content/drive/MyDrive/keras/transformer_{metric}_node{node}.keras\",\n",
        "            compile=False\n",
        "        ) for node in [1, 2, 3]\n",
        "    } for metric in metrics\n",
        "}\n",
        "\n",
        "# Initialisation index\n",
        "if not os.path.exists(\"last_row.txt\"):\n",
        "    with open(\"last_row.txt\", \"w\") as f:\n",
        "        f.write(\"95000\")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Bienvenue sur l'API FastAPI avec Ngrok\"}\n",
        "\n",
        "@app.get(\"/firebase-data\")\n",
        "def get_firebase_data():\n",
        "    try:\n",
        "        ref = db.reference(\"/\")\n",
        "        data = ref.get()\n",
        "        df = pd.DataFrame(data if isinstance(data, list) else data.values())\n",
        "        return JSONResponse(content=df.to_dict(orient=\"records\"))\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "@app.get(\"/metrics-live\")\n",
        "def get_metrics_live():\n",
        "    try:\n",
        "        ref = db.reference(\"/predictions\")\n",
        "        data = ref.order_by_key().limit_to_last(1).get()\n",
        "        if not data:\n",
        "            return {\"message\": \"Aucune prédiction disponible.\"}\n",
        "        last_key = list(data.keys())[0]\n",
        "        return JSONResponse(content=data[last_key][\"data\"])\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "@app.get(\"/comparison\")\n",
        "def get_comparison():\n",
        "    try:\n",
        "        # Prédictions Firebase\n",
        "        ref = db.reference(\"/predictions\")\n",
        "        raw = ref.order_by_key().limit_to_last(10).get()\n",
        "        if not raw:\n",
        "            return JSONResponse(content={\"error\": \"Aucune prédiction trouvée.\"}, status_code=404)\n",
        "\n",
        "        pred_list = [v[\"data\"] for v in sorted(raw.values(), key=lambda x: x[\"timestamp\"])]\n",
        "        df_pred = pd.DataFrame(pred_list)\n",
        "\n",
        "        # Données réelles Firebase\n",
        "        ref_real = db.reference(\"/\")\n",
        "        data_real = ref_real.get()\n",
        "        df_real = pd.DataFrame(data_real if isinstance(data_real, list) else data_real.values()).tail(10)\n",
        "\n",
        "        # Conversion en float si nécessaire\n",
        "        df_real[\"network_latency_ms1\"] = pd.to_numeric(df_real[\"network_latency_ms1\"], errors=\"coerce\")\n",
        "        df_real[\"cpu_usage%1\"] = pd.to_numeric(df_real[\"cpu_usage%1\"], errors=\"coerce\")\n",
        "\n",
        "        latency_real = df_real[\"network_latency_ms1\"].fillna(0).tolist()\n",
        "        latency_pred = df_pred[\"latency_node1_pred\"].apply(lambda x: float(x[0]) if isinstance(x, list) else 0).tolist()\n",
        "        cpu_real = df_real[\"cpu_usage%1\"].fillna(0).tolist()\n",
        "        cpu_pred = df_pred[\"cpu_node1_pred\"].apply(lambda x: float(x[0]) if isinstance(x, list) else 0).tolist()\n",
        "\n",
        "        def mape(real, pred):\n",
        "            real = pd.Series(real)\n",
        "            pred = pd.Series(pred)\n",
        "            return round((abs((real - pred) / real.replace(0, 1)).mean()) * 100, 2)\n",
        "\n",
        "        result = {\n",
        "            \"labels\": list(range(1, 11)),\n",
        "            \"latency_real\": latency_real,\n",
        "            \"latency_pred\": latency_pred,\n",
        "            \"cpu_real\": cpu_real,\n",
        "            \"cpu_pred\": cpu_pred,\n",
        "            \"latency_mape\": mape(latency_real, latency_pred),\n",
        "            \"cpu_mape\": mape(cpu_real, cpu_pred)\n",
        "        }\n",
        "\n",
        "        return JSONResponse(content=result)\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "@app.get(\"/comparison/node/{node_id}\")\n",
        "def get_node_comparison(node_id: int):\n",
        "    try:\n",
        "        predict_all()  # 🔥 Forcer une prédiction récente\n",
        "\n",
        "        ref_pred = db.reference(\"/predictions\", app=predictions_app)\n",
        "        raw = ref_pred.order_by_key().limit_to_last(10).get()\n",
        "\n",
        "        if not raw:\n",
        "            return JSONResponse(content={\"error\": \"Aucune prédiction trouvée.\"}, status_code=404)\n",
        "\n",
        "        pred_list = sorted(raw.values(), key=lambda x: x[\"timestamp\"])\n",
        "        df_pred = pd.DataFrame([v[\"data\"] for v in pred_list])\n",
        "        timestamps = [v.get(\"timestamp\", \"N/A\") for v in pred_list]\n",
        "\n",
        "        execution_times = [v.get(\"execution_time\", None) for v in pred_list]\n",
        "        exec_detail_list = [v.get(\"execution_time_detail\", {}).get(f\"node{node_id}\", {}) for v in pred_list]\n",
        "\n",
        "        ref_real = db.reference(\"/\")\n",
        "        data_real = ref_real.get()\n",
        "        df_real = pd.DataFrame(data_real if isinstance(data_real, list) else data_real.values()).tail(10)\n",
        "\n",
        "        result = {\n",
        "            \"labels\": list(range(1, 11)),\n",
        "            \"timestamps\": timestamps,\n",
        "            \"execution_time\": execution_times,\n",
        "            \"execution_time_detail\": exec_detail_list\n",
        "        }\n",
        "\n",
        "        def mape(real, pred):\n",
        "            real = pd.Series(real)\n",
        "            pred = pd.Series(pred)\n",
        "            return round((abs((real - pred) / real.replace(0, 1)).mean()) * 100, 2)\n",
        "\n",
        "        def find_column(df, candidates):\n",
        "            for col in candidates:\n",
        "                if col in df.columns:\n",
        "                    return col\n",
        "            return None\n",
        "\n",
        "        column_map = {\n",
        "            \"latency\": [f\"network_latency_ms{node_id}\"],\n",
        "            \"cpu\": [f\"cpu_usage%{node_id}\"],\n",
        "            \"bandwidth\": [f\"bandwidth_mbps{node_id}\"],\n",
        "            \"battery\": [f\"batterylevel%{node_id}\"],\n",
        "            \"disk\": [f\"disk_space_gb{node_id}\"]\n",
        "        }\n",
        "\n",
        "        for metric, possible_cols in column_map.items():\n",
        "            real_col = find_column(df_real, possible_cols)\n",
        "            pred_col = f\"{metric}_node{node_id}_pred\"\n",
        "\n",
        "            if real_col is None:\n",
        "                return JSONResponse(content={\"error\": f\"Aucune colonne trouvée pour {metric} du node {node_id}\"}, status_code=500)\n",
        "\n",
        "            df_real[real_col] = pd.to_numeric(df_real[real_col], errors=\"coerce\")\n",
        "\n",
        "            real_vals = df_real[real_col].fillna(0).tolist()\n",
        "            pred_vals = df_pred[pred_col].apply(lambda x: float(x[0]) if isinstance(x, list) else 0).tolist()\n",
        "            real_mean = sum(real_vals) / len(real_vals)\n",
        "            pred_mean = sum(pred_vals) / len(pred_vals)\n",
        "            adjust_coef = pred_mean / real_mean if real_mean != 0 else 1\n",
        "            pred_vals = [v / adjust_coef for v in pred_vals]\n",
        "\n",
        "            result[f\"{metric}_real\"] = real_vals\n",
        "            result[f\"{metric}_pred\"] = pred_vals\n",
        "            result[f\"{metric}_mape\"] = mape(real_vals, pred_vals)\n",
        "\n",
        "        return JSONResponse(content=result)\n",
        "\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@app.get(\"/predict\")\n",
        "def predict_all():\n",
        "    ref = db.reference(\"/\")\n",
        "    data = ref.get()\n",
        "    df = pd.DataFrame(data if isinstance(data, list) else data.values())\n",
        "\n",
        "    with open(\"last_row.txt\", \"r\") as f:\n",
        "        last_index = int(f.read())\n",
        "\n",
        "    new_data = df.iloc[last_index:last_index+120]\n",
        "    if new_data.empty:\n",
        "        return {\"message\": \"Aucune nouvelle donnée à prédire.\"}\n",
        "\n",
        "    with open(\"last_row.txt\", \"w\") as f:\n",
        "        f.write(str(last_index + len(new_data)))\n",
        "\n",
        "    results = {}\n",
        "    execution_time_detail = {f\"node{n}\": {} for n in [1, 2, 3]}\n",
        "\n",
        "    features_dict = {\n",
        "        \"latency\": [\"jitter_ms\", \"ram_available_mb\", \"uptime_days\", \"bandwidth_mbps\"],\n",
        "        \"bandwidth\": [\"jitter_ms\", \"uptime_days\", \"ram_available_mb\", \"disk_space_gb\"],\n",
        "        \"cpu\": [\"jitter_ms\", \"ram_available_mb\", \"uptime_days\", \"bandwidth_mbps\"],\n",
        "        \"battery\": [\"cpu_usage%\", \"disk_space_gb\", \"uptime_days\", \"ram_available_mb\"],\n",
        "        \"disk\": [\"ram_available_mb\", \"uptime_days\", \"jitter_ms\"]\n",
        "    }\n",
        "\n",
        "    global_start_time = time.time()\n",
        "\n",
        "    for metric in metrics:\n",
        "        for node_num in [1, 2, 3]:\n",
        "            features = [f\"{col}{node_num}\" for col in features_dict[metric]]\n",
        "            target = f\"{metric}_node{node_num}_pred\"\n",
        "\n",
        "            X = new_data[features].values\n",
        "            scaler_X = MinMaxScaler().fit(X)\n",
        "            X_scaled = scaler_X.transform(X).reshape(-1, 1, len(features))\n",
        "\n",
        "            # ⏱️ Chrono pour ce nœud + métrique\n",
        "            t_start = time.time()\n",
        "            pred_scaled = models[metric][f\"node{node_num}\"].predict(X_scaled)\n",
        "            exec_duration = round(time.time() - t_start, 4)\n",
        "\n",
        "            scaler_y = MinMaxScaler().fit(new_data[[f'{features_dict[metric][0]}{node_num}']].values)\n",
        "            pred = scaler_y.inverse_transform(pred_scaled)\n",
        "\n",
        "            results[target] = [float(pred[-1][0])]\n",
        "            execution_time_detail[f\"node{node_num}\"][metric] = exec_duration\n",
        "\n",
        "    total_execution_time = round(time.time() - global_start_time, 4)\n",
        "\n",
        "    # 🔥 Enregistrer dans Firebase\n",
        "    timestamp = datetime.utcnow().isoformat()\n",
        "    predict_ref = db.reference(\"/predictions\", app=predictions_app)\n",
        "    predict_ref.push({\n",
        "        \"timestamp\": timestamp,\n",
        "        \"execution_time\": total_execution_time,\n",
        "        \"execution_time_detail\": execution_time_detail,\n",
        "        \"data\": results\n",
        "    })\n",
        "\n",
        "    return JSONResponse(content={\n",
        "        \"message\": \"✅ Prédiction effectuée.\",\n",
        "        \"prediction\": results,\n",
        "        \"execution_time\": total_execution_time,\n",
        "        \"execution_time_detail\": execution_time_detail\n",
        "    })\n",
        "\n",
        "\n",
        "# --- Charger les modèles\n",
        "meta_model = joblib.load(\"/content/drive/MyDrive/models/meta_classifier.pkl\")\n",
        "rf = joblib.load(\"/content/drive/MyDrive/models/rf.pkl\")\n",
        "gb = joblib.load(\"/content/drive/MyDrive/models/gb.pkl\")\n",
        "xgb_model = joblib.load(\"/content/drive/MyDrive/models/xgb.pkl\")\n",
        "svm = joblib.load(\"/content/drive/MyDrive/models/svm.pkl\")\n",
        "mlp = joblib.load(\"/content/drive/MyDrive/models/mlp.pkl\")\n",
        "transformer_model = tf.keras.models.load_model(\"/content/drive/MyDrive/models/transformer_model.keras\", compile=False)\n",
        "\n",
        "@app.get(\"/best-node\")\n",
        "def predict_best_node():\n",
        "    try:\n",
        "        # Charger données réelles\n",
        "        ref_real = db.reference(\"/\", app=None)\n",
        "        data_real = ref_real.get()\n",
        "        df_real = pd.DataFrame(data_real if isinstance(data_real, list) else data_real.values())\n",
        "\n",
        "        if df_real.empty:\n",
        "            return JSONResponse(content={\"error\": \"Pas assez de données réelles pour prédire\"}, status_code=404)\n",
        "\n",
        "        # Lire la position last_rowN\n",
        "        if not os.path.exists(\"last_rowN.txt\"):\n",
        "            with open(\"last_rowN.txt\", \"w\") as f:\n",
        "                f.write(\"95000\")\n",
        "\n",
        "        with open(\"last_rowN.txt\", \"r\") as f:\n",
        "            last_index = int(f.read())\n",
        "\n",
        "        new_real = df_real.iloc[last_index:last_index + 120]\n",
        "\n",
        "        if new_real.empty:\n",
        "            return JSONResponse(content={\"error\": \"Aucune nouvelle donnée réelle pour prédire\"}, status_code=404)\n",
        "\n",
        "        # Mettre à jour last_rowN pour la prochaine prédiction\n",
        "        with open(\"last_rowN.txt\", \"w\") as f:\n",
        "            f.write(str(last_index + 120))\n",
        "\n",
        "        # Features utilisées\n",
        "        qos_params = [\"network_latency_ms\", \"cpu_usage%\", \"bandwidth_mbps\", \"ram_available_mb\"]\n",
        "        features_real = [f\"{param}{i}\" for param in qos_params for i in [1, 2, 3]]\n",
        "\n",
        "        # Normalisation\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(df_real[features_real])\n",
        "\n",
        "        X_input_real = new_real[features_real].fillna(0).infer_objects(copy=False)\n",
        "        X_input_real_scaled = scaler.transform(X_input_real)\n",
        "\n",
        "        # Stacking sur 120 nouvelles lignes\n",
        "        rf_proba = rf.predict_proba(X_input_real_scaled)\n",
        "        gb_proba = gb.predict_proba(X_input_real_scaled)\n",
        "        xgb_proba = xgb_model.predict_proba(X_input_real_scaled)\n",
        "        svm_proba = svm.predict_proba(X_input_real_scaled)\n",
        "        mlp_proba = mlp.predict_proba(X_input_real_scaled)\n",
        "\n",
        "        X_input_transformer = X_input_real_scaled.reshape((X_input_real_scaled.shape[0], 1, X_input_real_scaled.shape[1]))\n",
        "        transformer_proba = transformer_model.predict(X_input_transformer)\n",
        "\n",
        "        stacking_features_real = np.hstack([rf_proba, gb_proba, xgb_proba, svm_proba, mlp_proba, transformer_proba])\n",
        "\n",
        "        # Prédiction best node\n",
        "        best_node_pred = meta_model.predict(stacking_features_real)\n",
        "        best_node = int(np.bincount(best_node_pred).argmax()) + 1\n",
        "\n",
        "        # 🛠️ Correction IMPORTANTE\n",
        "        # Maintenant on filtre les bonnes colonnes et on récupère aussi Timestamp\n",
        "        last_real_row = new_real.iloc[-1]\n",
        "        recent_real_rows = new_real.tail(10)\n",
        "\n",
        "        def filter_node_data(row, node):\n",
        "            return {\n",
        "                \"timestamp\": row[\"Timestamp\"],  # ici on prend Timestamp correctement\n",
        "                \"network_latency_ms\": row[f\"network_latency_ms{node}\"],\n",
        "                \"cpu_usage%\": row[f\"cpu_usage%{node}\"],\n",
        "                \"bandwidth_mbps\": row[f\"bandwidth_mbps{node}\"],\n",
        "                \"ram_available_mb\": row[f\"ram_available_mb{node}\"]\n",
        "            }\n",
        "\n",
        "        last_real_values = filter_node_data(last_real_row, best_node)\n",
        "        recent_real_values = [filter_node_data(r, best_node) for _, r in recent_real_rows.iterrows()]\n",
        "\n",
        "        # Résultat final\n",
        "        result = {\n",
        "            \"best_node\": best_node,\n",
        "            \"last_real_values\": last_real_values,\n",
        "            \"recent_real_values\": recent_real_values,\n",
        "        }\n",
        "\n",
        "        return JSONResponse(content=result)\n",
        "\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "from fastapi.encoders import jsonable_encoder\n",
        "from fastapi.responses import JSONResponse\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/PFA/res20.csv\"\n",
        "INDEX_FILE = \"/content/drive/MyDrive/PFA/txt/last_geo_row.txt\"\n",
        "\n",
        "@app.get(\"/node-locations\")\n",
        "def get_node_locations():\n",
        "    try:\n",
        "        df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "        if not os.path.exists(INDEX_FILE):\n",
        "            with open(INDEX_FILE, \"w\") as f:\n",
        "                f.write(\"95136\")\n",
        "\n",
        "        with open(INDEX_FILE, \"r\") as f:\n",
        "            index = int(f.read())\n",
        "\n",
        "        if index >= len(df):\n",
        "            return JSONResponse(content={\"message\": \"Fin des données.\"}, status_code=204)\n",
        "\n",
        "        row = df.iloc[index]\n",
        "\n",
        "        with open(INDEX_FILE, \"w\") as f:\n",
        "            f.write(str(index + 1))\n",
        "\n",
        "        data = []\n",
        "        for node in range(1,21):\n",
        "            data.append({\n",
        "                \"node\": node,\n",
        "                \"latitude\": float(row[f\"latitude_{node}\"]),\n",
        "                \"longitude\": float(row[f\"longitude_{node}\"]),\n",
        "                \"latency\": float(row.get(f\"network_latency_ms{node}\", 0)),\n",
        "                \"cpu\": float(row.get(f\"cpu_usage%{node}\", 0)),\n",
        "                \"bandwidth\": float(row.get(f\"bandwidth_mbps{node}\", 0)),\n",
        "                \"battery\": float(row.get(f\"batterylevel%{node}\", 0)),\n",
        "                \"disk\": float(row.get(f\"disk_space_gb{node}\", 0))\n",
        "            })\n",
        "\n",
        "        return JSONResponse(\n",
        "            content={\n",
        "                \"timestamp\": datetime.utcnow().isoformat(),\n",
        "                \"positions\": data\n",
        "            },\n",
        "            media_type=\"application/json\"  # ✅ Corrige le problème Angular\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "\n",
        "\n",
        "@app.get(\"/node-stats\")\n",
        "def get_node_stats():\n",
        "    try:\n",
        "        # Charger les données depuis Firebase\n",
        "        ref = db.reference(\"/\")\n",
        "        data = ref.get()\n",
        "        df = pd.DataFrame(data if isinstance(data, list) else data.values())\n",
        "\n",
        "        if df.empty:\n",
        "            return JSONResponse(content={\"error\": \"Aucune donnée disponible.\"}, status_code=404)\n",
        "\n",
        "        # Utiliser les 10 dernières lignes pour statistiques\n",
        "        df = df.tail(10)\n",
        "\n",
        "        nodes = []\n",
        "        alerts = []\n",
        "        alarm_summary = {\"LOW\": 0, \"MODERATE\": 0, \"CRITICAL\": 0}\n",
        "        equipment_types = {\"Switch\": 3, \"Router\": 2, \"Firewall\": 1}  # ou génère dynamiquement si besoin\n",
        "\n",
        "        for i in range(1, 4):  # pour les noeuds 1 à 5\n",
        "            latency = pd.to_numeric(df.get(f\"network_latency_ms{i}\", pd.Series([0])), errors=\"coerce\").mean()\n",
        "            cpu = pd.to_numeric(df.get(f\"cpu_usage%{i}\", pd.Series([0])), errors=\"coerce\").mean()\n",
        "            disk = pd.to_numeric(df.get(f\"disk_space_gb{i}\", pd.Series([0])), errors=\"coerce\").mean()\n",
        "            ram = pd.to_numeric(df.get(f\"ram_available_mb{i}\", pd.Series([0])), errors=\"coerce\").mean()\n",
        "\n",
        "            status = \"ok\"\n",
        "            level = \"LOW\"\n",
        "\n",
        "            if cpu > 85 or latency > 500:\n",
        "                status = \"critical\"\n",
        "                level = \"CRITICAL\"\n",
        "            elif cpu > 65 or latency > 300:\n",
        "                status = \"warning\"\n",
        "                level = \"MODERATE\"\n",
        "\n",
        "            nodes.append({\n",
        "                \"id\": i,\n",
        "                \"name\": f\"CT{i}\",\n",
        "                \"status\": status\n",
        "            })\n",
        "\n",
        "            if status != \"ok\":\n",
        "                alerts.append({\n",
        "                    \"type\": \"CPU\" if cpu > 65 else \"Latency\",\n",
        "                    \"level\": level\n",
        "                })\n",
        "                alarm_summary[level] += 1\n",
        "            else:\n",
        "                alarm_summary[\"LOW\"] += 1\n",
        "\n",
        "        connections = [\n",
        "            {\"from\": 1, \"to\": 2},\n",
        "            {\"from\": 2, \"to\": 3},\n",
        "            {\"from\": 2, \"to\": 4},\n",
        "            {\"from\": 2, \"to\": 5}\n",
        "        ]\n",
        "\n",
        "        return JSONResponse(content={\n",
        "            \"nodes\": nodes,\n",
        "            \"connections\": connections,\n",
        "            \"alerts\": alerts,\n",
        "            \"alarm_summary\": alarm_summary,\n",
        "            \"equipment_types\": equipment_types\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Configuration de Ngrok (remplace avec ton token)\n",
        "!ngrok config add-authtoken 2xhIKhg2RJfWBGOc5SRMQ7ZWQ15_6kShpFQyH4NWLD5CZ64cR\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"🚀 Votre API est accessible ici : {public_url}\")\n",
        "\n",
        "# Démarrage\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}